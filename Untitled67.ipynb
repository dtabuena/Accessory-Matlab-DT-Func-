{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5LmhHTN8HrADm6Ytetezh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtabuena/Accessory-Matlab-DT-Func-/blob/master/Untitled67.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ib1nKxZMouzP"
      },
      "outputs": [],
      "source": [
        "\"\"\" CONFIGURE \"\"\"\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "import urllib\n",
        "import pandas as pd\n",
        "import scipy as sci\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import subprocess\n",
        "\n",
        "response = urllib.request.urlretrieve('https://raw.githubusercontent.com/dtabuena/Resources/main/Matplotlib_Config/Load_FS6.py','Load_FS6.py')\n",
        "%run Load_FS6.py\n",
        "trodes_dat_reader_loc = 'C:\\\\Users\\\\dennis.tabuena\\\\Dropbox (Gladstone)\\\\0_Projects\\\\LFP_Refactor\\\\Trodes_2-5-1_Windows64\\\\Resources\\\\TrodesToPython'\n",
        "os.chdir(trodes_dat_reader_loc)\n",
        "import readTrodesExtractedDataFile3 as trodes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def read_npy_metadata(npy_filename):\n",
        "    with open(npy_filename, 'rb') as f:\n",
        "        version = np.lib.format.read_magic(f)\n",
        "        np.lib.format._check_version(version)\n",
        "        shape, fortran_order, dtype = np.lib.format.read_array_header_1_0(f)\n",
        "        offset = f.tell()\n",
        "    return shape, dtype, offset\n",
        "\n",
        "def load_npy_to_memmap(npy_filename):\n",
        "    shape, dtype, offset = read_npy_metadata(npy_filename)\n",
        "    print(f'Loading {dtype} array shape:{shape}...')\n",
        "    with open(npy_filename, 'rb') as f:\n",
        "        array = np.memmap(f, dtype=dtype, mode='r+', offset=offset, shape=shape)\n",
        "    print('     complete.')\n",
        "    return array\n",
        "\n",
        "def display_filter(fir_coeff,rate):\n",
        "    # Plot the frequency response of the filter\n",
        "    w, h = sci.signal.freqz(fir_coeff, worN=8000)\n",
        "    fig,ax=plt.subplots(1,2,figsize=(4,2),dpi=300)\n",
        "    ax[0].plot(fir_coeff)\n",
        "    ax[0].set_title('Coefficients')\n",
        "    ax[1].plot(0.5 * rate * w / np.pi, np.abs(h), 'b')\n",
        "    ax[1].set_title('FIR Filter Frequency Response')\n",
        "    ax[1].set_xlabel('Frequency (Hz)')\n",
        "    ax[1].set_ylabel('Gain')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "def plot_sample_wave(lfp,sample_rate,times=[0,3],ref=None):\n",
        "    colors = ['k']*8 + ['r']*8 + ['k']*8 + ['r']*8\n",
        "    tic_indicies = np.arange(times[0]*sample_rate,times[1]*sample_rate)\n",
        "    time_sec = tic_indicies/sample_rate\n",
        "    waveform = lfp[tic_indicies,:]\n",
        "\n",
        "    num_chan = waveform.shape[1]\n",
        "    if ref is not None:\n",
        "        num_chan+= 1\n",
        "    fig,ax=plt.subplots(num_chan,1,figsize=(6,2),dpi=300)\n",
        "    for ch in np.arange(num_chan):\n",
        "        if ch>=waveform.shape[1]:\n",
        "            ax[ch].plot(time_sec,-ref[tic_indicies],linewidth=.2,color='c')\n",
        "            ax[ch].text(0,0,'ref')\n",
        "        else:\n",
        "            ax[ch].plot(time_sec,waveform[:,ch],linewidth=.2,color=colors[ch])\n",
        "            if ch%8 == 0 or ch+1==waveform.shape[1]:\n",
        "                ax[ch].text(0,0,f'ch.{int(ch+1)}',ha='right',va='center')\n",
        "        ax[ch].set_position([0,ch/num_chan,1,1/num_chan])\n",
        "        ax[ch].axis('off')\n",
        "    return\n",
        "\n",
        "def update_hdf_data(file_path, dataset_name, new_data,dtype=None):\n",
        "    \"\"\"\n",
        "    Update an existing dataset in an HDF5 file or create a new one if it doesn't exist.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path (str): Path to the HDF5 file.\n",
        "    - dataset_name (str): Name of the dataset to update or create.\n",
        "    - new_data (numpy.ndarray): Data to write into the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # Open an HDF5 file in read/write mode\n",
        "    with h5py.File(file_path, 'a') as hdf_file:\n",
        "        # Check if dataset already exists\n",
        "        if dataset_name in hdf_file:\n",
        "            # Dataset exists, update its contents\n",
        "            del hdf_file[dataset_name]\n",
        "            hdf_file[dataset_name] = new_data\n",
        "            print(f\"Dataset '{dataset_name}' updated.\")\n",
        "        else:\n",
        "            # Dataset does not exist, create a new one\n",
        "            if dtype is None:\n",
        "                dtype = new_data.dtype\n",
        "            hdf_file.create_dataset(dataset_name, data=new_data, dtype=dtype)\n",
        "            print(f\"Dataset '{dataset_name}' '{dtype}' created with new data.\")\n",
        "        hdf_file.close()\n",
        "\n",
        "def list_hdf5_dataset_names(file_path):\n",
        "    \"\"\"\n",
        "    List all dataset names in an HDF5 file.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path (str): Path to the HDF5 file.\n",
        "    \"\"\"\n",
        "    with h5py.File(file_path, 'r') as hdf_file:\n",
        "        def print_dataset_name(name, obj):\n",
        "            if isinstance(obj, h5py.Dataset):\n",
        "                print(name, obj.dtype)\n",
        "\n",
        "\n",
        "        hdf_file.visititems(print_dataset_name)\n",
        "\n",
        "def load_hdf5_to_var(hdf5_filename, dataset_name):\n",
        "    \"\"\"\n",
        "    Load a dataset from an HDF5 file into a NumPy memmap array.\n",
        "\n",
        "    Parameters:\n",
        "    - hdf5_filename (str): Path to the HDF5 file.\n",
        "    - dataset_name (str): Name of the dataset within the HDF5 file.\n",
        "\n",
        "    Returns:\n",
        "    - memmap_array (numpy.memmap): Memmap array containing the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    with h5py.File(hdf5_filename, 'r') as hdf_file:\n",
        "        # Check if the dataset exists\n",
        "        if dataset_name in hdf_file:\n",
        "            dataset = hdf_file[dataset_name]\n",
        "            if dataset.ndim == 0:\n",
        "                # Dataset is scalar, return the single value\n",
        "                data = dataset[()]\n",
        "            else:\n",
        "                # Dataset is not scalar, apply slicing if start and end are specified\n",
        "                data = dataset[:]\n",
        "            return data\n",
        "        else:\n",
        "            raise KeyError(f\"Dataset '{dataset_name}' not found in the file.\")\n",
        "    print('Loading complete.')\n",
        "    return data\n",
        "\n",
        "def load_hdf5_to_memmap(hdf5_filename, dataset_name):\n",
        "    \"\"\"\n",
        "    Load a dataset from an HDF5 file into a NumPy memmap array.\n",
        "\n",
        "    Parameters:\n",
        "    - hdf5_filename (str): Path to the HDF5 file.\n",
        "    - dataset_name (str): Name of the dataset within the HDF5 file.\n",
        "\n",
        "    Returns:\n",
        "    - memmap_array (numpy.memmap): Memmap array containing the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    with h5py.File(hdf5_filename, 'r') as f:\n",
        "        # Read metadata from HDF5 attributes or dataset properties\n",
        "        dataset = f[dataset_name]\n",
        "        shape = dataset.shape\n",
        "        dtype = dataset.dtype\n",
        "\n",
        "        print(f'Loading {dtype} array shape: {shape} from HDF5...')\n",
        "\n",
        "        if shape == ():  # Check if the dataset is scalar\n",
        "            # Create a temporary file\n",
        "            temp_file = tempfile.NamedTemporaryFile(delete=False)\n",
        "            temp_file.close()\n",
        "\n",
        "            # Create memmap array for a single scalar value\n",
        "            memmap_array = np.memmap(temp_file.name, dtype=dtype, mode='w+', shape=(1,))\n",
        "            memmap_array[0] = float(dataset[()])\n",
        "        else:\n",
        "            # Create a temporary file\n",
        "            temp_file = tempfile.NamedTemporaryFile(delete=False)\n",
        "            temp_file.close()\n",
        "\n",
        "            # Create memmap array and write data to it\n",
        "            memmap_array = np.memmap(temp_file.name, dtype=dtype, mode='w+', shape=shape)\n",
        "            memmap_array[:] = dataset[:]\n",
        "        f.close()\n",
        "    print('Loading complete.')\n",
        "\n",
        "    return memmap_array"
      ],
      "metadata": {
        "id": "jB4loeA7ozo0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Session Info \"\"\"\n",
        "dest_dir = r'\\\\hive.gladstone.internal\\Huang-LFP\\TabuenaLFP\\TRODE_EXPORTS\\RAW_DATA_X'\n",
        "export_func_location = r'\\\\hive\\Huang-LFP\\TabuenaLFP\\SpikeGadgets\\Trodes_2-5-1_Windows64\\Trodes_2-5-1_Windows64\\trodesexport.exe'\n",
        "dot_rec_file_loc = r'\\\\hive\\Huang-LFP\\TabuenaLFP\\ZL04_6201\\ZL04_6201.rec'\n",
        "animal_session = os.path.basename(dot_rec_file_loc).replace('.rec','')\n",
        "\n",
        "dot_dat_file_loc = r'\\\\hive.gladstone.internal\\Huang-LFP\\TabuenaLFP\\TRODE_EXPORTS\\RAW_DATA\\ZL04_6201.raw\\ZL04_6201.raw_group0.dat'\n",
        "my_analysis_dir = r'\\\\hive.gladstone.internal\\Huang-LFP\\TabuenaLFP\\ReAnalyze'\n",
        "os.chdir(my_analysis_dir)\n",
        "session_hdf_file=os.path.join(my_analysis_dir,animal_session+'.hdf5')\n"
      ],
      "metadata": {
        "id": "e5CPXTino_UD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lfp_disk_array = load_hdf5_to_memmap(session_hdf_file, 'smooth_envelope')\n",
        "downsampled_rate = load_hdf5_to_memmap(session_hdf_file, 'downsampled_rate')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moMJpbBko4Gd",
        "outputId": "d9502489-a91d-49ee-a15a-2cbc609594fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading float16 array shape: (12637483, 32) from HDF5...\n",
            "Loading complete.\n",
            "Loading int32 array shape: () from HDF5...\n",
            "Loading complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zl_dot_dat_dir = r'\\\\hive.gladstone.internal\\Huang-LFP\\TabuenaLFP\\ZL04_6201\\ZL04_6201.LFP'\n",
        "os.listdir(zl_dot_dat_dir)\n",
        "file_list = list()\n",
        "for root,_,files in os.walk(zl_dot_dat_dir):\n",
        "    for f in files:\n",
        "        if 'time' not in f:\n",
        "            file_list.append(os.path.join(root,f))"
      ],
      "metadata": {
        "id": "qQy6-EImsLcb"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C84w16Jxkwp",
        "outputId": "76526c0a-99ec-4c39-eb53-e7e1d0b40184"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt10ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt11ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt12ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt13ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt14ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt15ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt16ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt17ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt18ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt19ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt1ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt20ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt21ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt22ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt23ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt24ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt25ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt26ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt27ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt28ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt29ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt2ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt30ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt31ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt32ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt3ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt4ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt5ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt6ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt7ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt8ch1.dat',\n",
              " '\\\\\\\\hive.gladstone.internal\\\\Huang-LFP\\\\TabuenaLFP\\\\ZL04_6201\\\\ZL04_6201.LFP\\\\ZL04_6201.LFP_nt9ch1.dat']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(zl_dot_dat_dir)\n",
        "dot_dat_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxf0cl7IuIIe",
        "outputId": "ae091dd8-bc73-4439-f1fc-0fcc6ceb8022"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'description': 'LFP data for one channel',\n",
              " 'byte_order': 'little endian',\n",
              " 'original_file': 'ZL04_6201.rec',\n",
              " 'ntrode_id': '10',\n",
              " 'ntrode_channel': '1',\n",
              " 'clock rate': '30000',\n",
              " 'voltage_scaling': '0.195',\n",
              " 'decimation': '1',\n",
              " 'system_time_at_creation': '1673309131113',\n",
              " 'timestamp_at_creation': '858897',\n",
              " 'first_timestamp': '888740',\n",
              " 'reference': 'off',\n",
              " 'low_pass_filter': '6000',\n",
              " 'clockrate': '30000',\n",
              " 'trodes_version': '1.7.4',\n",
              " 'compile_date': 'Jun 20 2018',\n",
              " 'compile_time': '14:26:30',\n",
              " 'qt_version': '5.9.1',\n",
              " 'commit_tag': 'heads/Release_1.7.4-0-g65dde944',\n",
              " 'controller_firmware': '2.2',\n",
              " 'headstage_firmware': '3.4',\n",
              " 'autosettle': '0',\n",
              " 'smartref': '0',\n",
              " 'gyro': '0',\n",
              " 'accelerometer': '0',\n",
              " 'magnetometer': '0',\n",
              " 'time_offset': '0',\n",
              " 'fields': '<voltage int16>',\n",
              " 'data': array([(344,), (335,), (345,), ..., (418,), (481,), (456,)],\n",
              "       dtype=[('voltage', '<i2')])}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_list=dict()\n",
        "for f in tqdm(file_list):\n",
        "    dot_dat_data = trodes.readTrodesExtractedDataFile(f)\n",
        "    ch = dot_dat_data['ntrode_id']\n",
        "    print(ch)\n",
        "    data_list[ch] = dot_dat_data['data'].astype('float16') * float(dot_dat_data['voltage_scaling'])\n",
        "\n"
      ],
      "metadata": {
        "id": "R2fflljWuA1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_analysis_dir = r'\\\\hive.gladstone.internal\\Huang-LFP\\TabuenaLFP\\ReAnalyze'"
      ],
      "metadata": {
        "id": "wpAeCVuWx4eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_channels = len(data_list)\n",
        "num_samples = len(data_list[ch])\n",
        "memmap_filename = os.path.join(my_analysis_dir,'zll_disk_array.dat')\n",
        "zll_disk_array = np.memmap(memmap_filename, dtype='float16', mode='w+', shape=(num_samples,num_channels))\n",
        "\n",
        "for ch,val in data_list.items():\n",
        "    zll_disk_array[:,int(ch)] = val\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "iuMsp0Ouxx8o",
        "outputId": "771a5276-811d-4140-ec49-e2ffc5e7939e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 32 is out of bounds for axis 1 with size 32",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[56], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m zll_disk_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmemmap(memmap_filename, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m'\u001b[39m, shape\u001b[38;5;241m=\u001b[39m(num_samples,num_channels))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ch,val \u001b[38;5;129;01min\u001b[39;00m data_list\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m----> 7\u001b[0m     zll_disk_array[:,\u001b[38;5;28mint\u001b[39m(ch)] \u001b[38;5;241m=\u001b[39m val\n",
            "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\notebook\\Lib\\site-packages\\numpy\\core\\memmap.py:335\u001b[0m, in \u001b[0;36mmemmap.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m--> 335\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(index)\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(res) \u001b[38;5;129;01mis\u001b[39;00m memmap \u001b[38;5;129;01mand\u001b[39;00m res\u001b[38;5;241m.\u001b[39m_mmap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    337\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mndarray)\n",
            "\u001b[1;31mIndexError\u001b[0m: index 32 is out of bounds for axis 1 with size 32"
          ]
        }
      ]
    }
  ]
}